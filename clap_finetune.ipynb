{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Finetune CLAP for PAGES Evaluation </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During evaluation of the PAGES model, we determined that using CLAP to embed both the input/source text and the generated background music did not yield great results. After consideration, we determined that this metric did not make sense, and the task was out of domain for the CLAP model, which was trained to embed audio and corresponding text labels into similar places in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For emotion fine-tuning: [EMOPIA](https://annahung31.github.io/EMOPIA/), [Music Emotion Recognition](https://github.com/juansgomez87/vis-mtg-mer)\n",
    "\n",
    "For script fine-tuning: [VGMIDI](https://github.com/lucasnfe/vgmidi), [YM2413-MDB](https://jech2.github.io/YM2413-MDB/), could also consider using Spotify API to compare music and lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, ClapModel  #AutoProcessor\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
